实验：
    (1) grpc相关运行实验
    (2) 容器相关实验，包括如何解析，生成容器镜像（docker与containerd）
    (3) k8s,knative的实验，包含面对各场景下应该配置哪些参数与各场景下配置哪些插件
    (4) 在熟悉nginx与代理服务的实验
    (5) go-wapper具体的使用实验    
相关知识点：
    (1)firecraker的相关接口；k8s的ori接口，containerd的接口与firecracker接口的交互
    (2) 容器调用是否也可以像firecracker那样调用接口完成


源码：
    (1) 内存部分扫尾
    (2) 进程部分精细加稍微
 
文件:
    (1) 实验与各涉及工具的使用
    (2) 内核源码



faasnap：
	1. 构建了多个网络对
	* 整个交流过为测试程序通过gprc请求，到faasnap监控程序，监控程序监控所有请求
	2. 启动虚拟机，虚拟机启动时会监控请求
	3. 在物理机发起请求，虚拟机监控到请求，开始运行相关代码

	(*) 预热时（运行一个go线程），检测对应memfile的内存区域，每映射4k到内存便检测一次，记录每个页在检测的第几次出现。

vhive
kubelet设置：
    (1)/etc/default/kubelet添加
    KUBELET_EXTRA_ARGS="--v=%d --runtime-request-timeout=15m --container-runtime-endpoint=unix://%s --node-ip %s
--container-runtine-endpoint 
    (2) 初始化集群
    kubeadm init --ignore-preflight-errors=all --cri-socket unix://xxx --pod-network-cidr=xxx --kubernets-version xx
    (3) 删除所有节点上控制平面节点（control plane）默认添加的 taint，从而允许普通 pod 被调度到控制平面节点上
    kubectl taint nodes --all node-role.kubenets.io/control-plane-
    (4) 部署calico
    kubectl apply -f  xx.yaml
    (5) 等待其他节点
    kubectl wait --for xxxx
    (6) 配置metalLB裸金属环境的负载均衡

    对于所有的VM创建启动等都是通过firecraker-containerd/firecraker-control/client得到的。通构建请求，发送到firecraker-client
    最终都在ctriface/iface.go中调用
    而gvisor直接在/cri/gvisor/文件中便完成调用
    
 

执行逻辑：
	直接调用：
		构建了两个grpc服务，orchserve与fwdserve。这两个服务通过tcp连接，端口号分别是3333，3334。对于orchserve提供start, stopsingle, stop的服务。对于fwd，提供直接运行一个hello镜像的服务。

	CRI设置：
		firecraker与gvisor都创建了一个grpc server用于监听来自与k8s的容器服务请求（runtime与image），端口为vhive-cri.sock; 同时创建了一个criservice -- 其变量包含一个firecrakerserice（包含一个client：runtime）,两个client(runtime， image) 。client用于请求containerd.sock。
	
	firecarker/service.go  与 gvisor/service.go 有容器管理中修改的函数。
	未修改的函数位于cri/proxy.go, 直接转发至containerd。

	
	对于firecracker有一个coordinator: 包含一个funcInstance哈希表, orchestrator, snapshotmanager(地址/fccd/test/snapshots)； snapshotmanager包含一个snaoshots的snapshot的哈希表
	
容器运行大模型？？




